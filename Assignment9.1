Problem Statement

Predicting Survival in the Titanic Data Set
We will be using a decision tree to make predictions about the Titanic data set from
Kaggle. This data set provides information on the Titanic passengers and can be used to
predict whether a passenger survived or not.
Loading Data and modules
import numpy as np
import pandas as pd
import seaborn as sb
import matplotlib.pyplot as plt
import sklearn
from pandas import Series, DataFrame
from pylab import rcParams
from sklearn import preprocessing
from sklearn.linear_model import LogisticRegression
from sklearn.cross_validation import train_test_split
from sklearn import metrics

from sklearn.metrics import classification_report
Url=
https://raw.githubusercontent.com/BigDataGal/Python-for-Data-Science/master/titanic
-train.csv
titanic = pd.read_csv(url)
titanic.columns =
['PassengerId','Survived','Pclass','Name','Sex','Age','SibSp','Parch','Ticket','Fare','Cabin','E
mbarked']
You use only Pclass, Sex, Age, SibSp (Siblings aboard), Parch (Parents/children aboard),
and Fare to predict whether a passenger survived.

import numpy as np
import pandas as pd
import seaborn as sb
import matplotlib.pyplot as plt
import sklearn
from pandas import Series, DataFrame
from pylab import rcParams
from sklearn import preprocessing
from sklearn.linear_model import LogisticRegression
from sklearn.cross_validation import train_test_split
from sklearn import metrics
from sklearn.metrics import classification_report
from sklearn import tree


# Import data
train_df = pd.read_csv('https://raw.githubusercontent.com/BigDataGal/Python-for-Data-Science/master/titanic-train.csv')


# Viewing the data types and missing data
train_df.info()

# displaying first few lines of training data
train_df.head()

# Store target variable of training data in a other variable
survived_train = train_df.Survived

#store the train_df to data
data = train_df

# Viewing the data types and missing data
data.info()

# Preprocessing missing numerical variables
data['Age'] = data.Age.fillna(data.Age.median())
data['Fare'] = data.Fare.fillna(data.Fare.median())

# Viewing the data types and missing data
data.info()

# Encode the string data into binary form
#This can also be used with the help of label encoder
#converting the categorical variable into numerical
data = pd.get_dummies(data, columns=['Sex'], drop_first=True)
data.head()

#data[['Sex_male', 'Fare', 'Age','Pclass', 'SibSp']]
data.head()

X = data.iloc[:,[2,11,4,5,6]].values 
y = data.iloc[:,1]

# Splitting data set into Training set & Test set
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1/4, random_state=0)


# Feature scaling
from sklearn.preprocessing import  StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test  = sc.fit_transform(X_test)

# Fitting  DecisionTree
from sklearn.tree import DecisionTreeClassifier
classifier = DecisionTreeClassifier(criterion='entropy', random_state=0)
classifier.fit(X_train, y_train)
y_pred = classifier.predict(X_test)



from sklearn.metrics import confusion_matrix, accuracy_score
cm = confusion_matrix(y_test, y_pred)
acc_score = accuracy_score(y_test, y_pred)

print(classification_report(y_test, y_pred))

from sklearn.cross_validation import KFold
cv = KFold(n=len(X), n_folds = 10, random_state=0)


X_fold = data.iloc[:,[2,11,4,5,6]]
y_fold = data.iloc[:,1]

fold_accuracy = []

for train_fold, valid_fold in cv:
    train = X_fold.loc[train_fold]
    valid = X_fold.loc[valid_fold]
    
    train_y = y_fold.loc[train_fold]
    valid_y = y_fold.loc[valid_fold]
    
    model = classifier.fit(X = train, y = train_y)
    valid_acc = model.score(X = train, y = train_y)
    fold_accuracy.append(valid_acc)
    
print("Survival per fold: ", fold_accuracy, "\n")
print("Avg. Survival: ", sum(fold_accuracy)/len(fold_accuracy))

# First stratified K Fold
from sklearn.cross_validation import cross_val_score
score = cross_val_score(estimator = classifier, X=X, y=y, scoring="accuracy", cv=15)
print("Survival: ")
print(score)
print("Avg. Survival: ", score.mean())



